{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/al34n1x/DataScience/blob/master/6.Gestion_de_datos/Agregaci%C3%B3n_de_datos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Jb_FkSc60JDN","colab_type":"toc"},"source":[">[Agregación de datos y operaciones de grupo](#scrollTo=BFynk27lvXm6)\n","\n",">>[Actividades que veremos en este apartado](#scrollTo=EMGwZJuAv306)\n","\n",">>>[Mecánica del GroupBy](#scrollTo=F1ju7vADwWy1)\n","\n",">>>[Seleccionando una columna o subset de columnas](#scrollTo=_SAtsTx-1xjS)\n","\n",">>>[Agrupando con dicts y series](#scrollTo=SFpRZZb23nOj)\n","\n",">>>[Agrupación con funciones](#scrollTo=hNvhRjQg5IhT)\n","\n",">>>[Data Aggregation](#scrollTo=tCCi7PrE5ecH)\n","\n",">>>[Agregación de columna inteligente y de funciones múltiples](#scrollTo=rbeK_M1blP_0)\n","\n",">>[Apply](#scrollTo=K8sgtonjqmcI)\n","\n",">>>[Análisis de cuantiles y buckets](#scrollTo=mFfqx7P0sbeq)\n","\n",">>>[Rellenar valores perdidos con valores específicos de grupo](#scrollTo=tvKXddD9tV0C)\n","\n",">>>[Un ejemplito de transformación y correlación entre columnas](#scrollTo=eX6B4Plhxr8P)\n","\n",">>[Pivot Tables y tabulación cruzada](#scrollTo=3e5jR8qh0UMW)\n","\n",">>>[Tabulaciones cruzadas (crosstab)](#scrollTo=gprrjJ0m12nf)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"BFynk27lvXm6"},"source":["# Agregación de datos y operaciones de grupo\n","\n","La categorización de un conjunto de datos y la aplicación de una función a cada grupo, ya sea una agregación o transformación, es un componente crítico del trabajo de análisis de datos. Después de cargar, fusionar y preparar un conjunto de datos, es posible que debas calcular estadísticas de grupo o posiblemente tablas dinámicas para fines de informes o visualización. Pandas proporciona una interfaz de grupo flexible, que te permite cortar, y resumir conjuntos de datos de forma natural.\n","\n","Como verás, con la expresividad de Python y Pandas, podemos realizar operaciones grupales bastante complejas utilizando cualquier función que acepte un objeto Pandas o un array Numpy. "]},{"cell_type":"markdown","metadata":{"id":"EMGwZJuAv306"},"source":["## Actividades que veremos en este apartado\n","\n","* Dividir un Dataframe en pedazos usando una o más claves (en forma de funciones, matrices o nombres de columna de DataFrame).\n","\n","* Calcular estadísticas de resumen de grupo, como conteo, media o desviación estándar, o una función definida por el usuario.\n","\n","* Aplicar transformaciones como normalización, regresión lineal, clasificación o selección de subconjuntos.\n","\n","* Calcular tablas dinámicas y tabulaciones cruzadas.\n","\n","* Realizar análisis de cuantiles y otros análisis de grupos estadísticos."]},{"cell_type":"markdown","metadata":{"id":"F1ju7vADwWy1"},"source":["### Mecánica del GroupBy\n","\n","Existe un término conocido entre los analistas que describe operaciones de grupo, *split-apply-combine*.\n","\n","En la primera parte de este proceso dividimos Dataframes o series (split) en grupos basados en una o más keys. Una vez realizado la división, realizamos la función *apply* a cada grupo, produciendo un nuevo valor.\n","\n","Finalmente, tomamos el resultado de esas operaciones y las combinamos en un objeto.\n","\n","![alt text](https://raw.githubusercontent.com/al34n1x/DataScience/master/img/split-apply-combine.png)\n","\n","*Fuente: Python for Data Analysis, 2nd Edition*\n","\n"]},{"cell_type":"code","metadata":{"id":"NbOGS3vovXEr"},"source":["import pandas as pd\n","import numpy as np\n","df = pd.DataFrame({'key1' : ['a', 'a', 'b', 'b', 'a'],\n","                   'key2' : ['one', 'two', 'one', 'two', 'one'],\n","                   'data1' : np.random.randn(5),\n","                   'data2' : np.random.randn(5)})\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BxNpTDnDzAOt"},"source":["Supongamos que deseas calcular la *media* de la columna data1 usando las etiquetas de key1"]},{"cell_type":"code","metadata":{"id":"Ip7mww3yyqBE"},"source":["grouped = df['data1'].groupby(df['key1'])\n","print(grouped)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uTPAa6D8yrYF"},"source":["grouped.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5CsULB8zhw7"},"source":["Aquí agrupamos los datos usando dos claves, y la Serie resultante ahora tiene un índice jerárquico."]},{"cell_type":"code","metadata":{"id":"B9IChMQuzjNw"},"source":["media = df['data1'].groupby([df['key1'], df['key2']]).mean()\n","media"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Q09b12lzugW"},"source":["media.unstack()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvowjZ25z3RS"},"source":["En el siguiente ejemplo todo el grupo de keys son series:"]},{"cell_type":"code","metadata":{"id":"BPb4TiJa0N1L"},"source":["prov = np.array(['Buenos Aires', 'Buenos Aires', 'Córdoba', 'Córdoba', 'Tucumán'])\n","anios = np.array([2005, 2005, 2005, 2006, 2006])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5xjS2CeIVBSO"},"source":["df['data1']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-AeL8DO0SUE"},"source":["df['data1'].groupby([prov, anios]).mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_SAtsTx-1xjS"},"source":["### Seleccionando una columna o subset de columnas\n","\n","La indexación de un objeto **GroupBy** creado a partir de un DataFrame con un nombre de columna o matriz de nombres de columna, genera un subconjunto de columnas para la agregación."]},{"cell_type":"code","metadata":{"id":"Ock1N3TL2BWx"},"source":["df.groupby('key1')['data1'] # Equivalente a df['data1'].groupby(df['key1'])\n","df.groupby('key1')['data2'] # Equivalente a df[['data2']].groupby(df['key1'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dHeua-rw2ghN"},"source":["Especialmente para grandes conjuntos de datos, puede ser conveniente agregar solo unas pocas columnas. Por ejemplo, en el conjunto de datos anterior, para calcular promedios solo para la columna data2 y obtener el resultado como un DataFrame, podríamos escribir:"]},{"cell_type":"code","metadata":{"id":"Ei1BEtyD3OFc"},"source":["df.groupby(['key1', 'key2'])[['data2']].mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_Dgzoor3tU7"},"source":["El objeto devuelto por esta operación de indexación es un DataFrame agrupado.\n","\n","Será una lista o matriz o una Serie agrupada si solo se pasa un solo nombre de columna como escalar"]},{"cell_type":"code","metadata":{"id":"NWWVxT0p3y8v"},"source":["s_grouped = df.groupby(['key1', 'key2'])['data2']\n","s_grouped.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SFpRZZb23nOj"},"source":["### Agrupando con dicts y series\n","\n","Puede que necesites agrupar información existente en algo diferente a un arreglo. Consideremos el siguiente Dataframe:\n"]},{"cell_type":"code","metadata":{"id":"DZpC2fry32bX"},"source":["people = pd.DataFrame(np.random.randn(5, 5),\n","                      columns=['a', 'b', 'c', 'd', 'e'],\n","                      index=['Joe', 'Steve', 'Wes', 'Jim', 'Travis'])\n","people"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G7STkNov37MX"},"source":["people.iloc[2:3, [1, 2]] = np.nan # Agrega un par de NaN\n","people"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGWo6SRE4GbV"},"source":["Supongamos que tenemos una lista de columnas que corresponden a ese Dataframe y queremos realizar una operación **sum** entre las columnas por grupo"]},{"cell_type":"code","metadata":{"id":"Bjnm__b-4Rl6"},"source":["mapping = {'a': 'red', 'b': 'red', 'c': 'blue',\n","           'd': 'blue', 'e': 'red', 'f' : 'orange'}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IFGdcnqY4lzh"},"source":["Ahora podemos construir un arreglo a partir del Diccionario y se lo pasamos a la operación **groupby**, pero en cambio le pasamos directamente el dict como key.\n"]},{"cell_type":"code","metadata":{"id":"RjeylfWs4Uoe"},"source":["by_column = people.groupby(mapping, axis=1)\n","by_column.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hNvhRjQg5IhT"},"source":["### Agrupación con funciones\n","El uso de las funciones de Python es una forma más genérica de definir un mapeo de grupo en comparación con un Diccionario o una Serie. \n","\n","**Cualquier función que se pase como clave de grupo se llamará una vez por valor de índice**, y los valores de retorno se utilizarán como nombres de grupo. Más concretamente, consideremos el DataFrame de ejemplo de la sección anterior, que tiene los nombres de las personas como valores de índice. Supongamos que deseas agrupar por la longitud de los nombres; Si bien podrías calcular una matriz de longitudes de cadena, es más simple simplemente pasar la función `len`:"]},{"cell_type":"code","source":["people #Recordemos el Dataframe original"],"metadata":{"id":"Zm5hdJScnpi3"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27Aaa4WO5VPM"},"source":["people.groupby(len).sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"VoGwQuEvlL1H"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tCCi7PrE5ecH"},"source":["\n","\n","---\n","\n","\n","### Data Aggregation\n","Las agregaciones se refieren a cualquier transformación de datos que **produce valores escalares a partir de matrices**. Los ejemplos anteriores han utilizado varios de ellos, como el cálculo de promedio, la suma, etc. \n","\n","\n","Function name |\tDescription\n","------------- | -----------\n","count\t| Número de valores no-NA en el grupo\n","sum\t| Suma de valores no-NA\n","mean\t| Media de valores no-NA \n","median\t| Mediana aritmética de valores no-NA\n","std, var\t| Desviación y varianza estándar imparcial (denominador n - 1)\n","min, max\t| Mínimo y máximo de valores no-NA\n","prod\t| Producto de valores no-NA \n","first, last\t| Primer y último valores no-NA \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-HRjvU8BkhEw"},"source":["Puedes usar agregaciones de tu propio diseño y, además, llamar a cualquier método que también esté definido en el objeto agrupado. \n","\n","**Ejemplo**: Veamos por ejemplo un ejemplo de cálculo de cuantil sobre un dataframe agrupado.\n","\n","Si bien el cuantil no se implementa explícitamente para GroupBy, es un método de la Serie y, por lo tanto, está disponible para su uso.  Internamente, GroupBy corta eficientemente la serie, llama a **quantile()** para cada pieza y luego ensambla esos resultados en el objeto de resultado:"]},{"cell_type":"code","metadata":{"id":"j5xgcSpEkuCL"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iC8mpPB3kyss"},"source":["grouped = df.groupby('key1')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oXzHSQdBk1yf"},"source":["grouped['data1'].quantile(0.5)\n","\n","# Análisis del resultado:\n","# 1. Vemos que el cuantil del 50% para la clave \"a\", que tenia 3 valores, coincide con el valor del promedio de los extremos.\n","# 2. En el caso de la clave \"b\" que tenia dos valores, coincide con el promedio de ambos.\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6PN0m7golB7F"},"source":["Puedes notar que algunos métodos como **describe** también funcionan, aunque no son agregaciones, estrictamente hablando"]},{"cell_type":"code","metadata":{"id":"8BC4f_1AlC-Y"},"source":["grouped.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rbeK_M1blP_0"},"source":["### Agregación de columna inteligente y de funciones múltiples\n","\n","Volvamos al conjunto de datos de propinas de ejemplos anteriores. Después de cargarlo con read_csv, agregamos una columna de porcentaje de propina tip_pct"]},{"cell_type":"code","metadata":{"id":"k1WKcHx3lkaz"},"source":["prop = pd.read_csv('https://raw.githubusercontent.com/manquintana/Finlab-UADE/main/Ejercitacion/Datasets/tips.csv')\n","prop['tip_pct'] = prop['tip'] / prop ['total_bill']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cD_0UR4Ol43g"},"source":["prop[:6]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A9NcYdlfmD6B"},"source":["Como hemos visto, agregar una Serie o todas las columnas de un dataframe de datos es una cuestión de utilizar el agregado con la función deseada o llamar a un método como **mean** o **std**. \n","Sin embargo, es posible que desees agregar usando una función diferente dependiendo de la columna, o múltiples funciones a la vez. "]},{"cell_type":"code","metadata":{"id":"MXZCz3IDmS8c"},"source":["grouped = prop.groupby(['day', 'smoker'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MtbQj7SKmXHp"},"source":["grouped_pct = grouped['tip_pct']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vuJY47Qm03p"},"source":["Ten en cuenta que para estadísticas descriptivas como las de la Tabla que hemos compartido al comienzo, igual que cuando hicimos el agrupamiento por función **len** se puede pasar el nombre de la función como una cadena, en este caso **mean**"]},{"cell_type":"code","metadata":{"id":"aHa_6xHZmaRg"},"source":["grouped_pct.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una manera equivalente de escribir lo mismo es realizar una **agregación**, que agregará tantas columnas como se le indique a la funcion **.agg()**:"],"metadata":{"id":"iZlLCj0019W_"}},{"cell_type":"code","source":["grouped_pct.agg('mean')"],"metadata":{"id":"xkuxPJXf1wl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gSfupXLnE8P"},"source":["grouped_pct.agg(['min','max'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pyIqaqvsn0Zd"},"source":["def peak_to_peak(arr):      # Función de agregación propia \n","  return arr.max() - arr.min()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HqBj-xNqnPXK"},"source":["Si pasas una lista de funciones o nombres de funciones, obtiene un DataFrame con nombres de columnas tomados de las funciones."]},{"cell_type":"code","metadata":{"id":"EOHUzldHnRae"},"source":["grouped_pct.agg(['mean', 'std', peak_to_peak])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["¿Y si quisieramos ver también sobre cuántos datos se hace cada operación para cada key, qué agregaríamos?"],"metadata":{"id":"gEAkh4vT2vTV"}},{"cell_type":"code","source":["grouped_pct.agg(['mean', 'std', peak_to_peak, 'count']) # ¿Qué falta?"],"metadata":{"id":"psZ6KXyA21rw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yWgyXT6spYjh"},"source":["\n","\n","---\n","**Cambiar los nombres de columna resultantes de la agregación**:\n","\n","No se necesita aceptar los nombres que GroupBy le da a las columnas. Si pasas una lista de tuplas (nombre, función), el primer elemento de cada tupla se usará como los nombres de columna de DataFrame."]},{"cell_type":"code","metadata":{"id":"gF7C15LrpjbG"},"source":["grouped_pct.agg([('Promedio de tip%', 'mean'), ('Desvio de tip%', np.std)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDb9RIsnpuLQ"},"source":["Con un DataFrame tienes más opciones, ya que puedes especificar una lista de funciones para aplicar a todas las columnas o diferentes funciones por columna.\n","\n","Para comenzar, supongamos que deseamos calcular las mismas tres estadísticas para las columnas tip_pct y total_bill"]},{"cell_type":"code","metadata":{"id":"JGLZlt6Kp2NW"},"source":["columnas = ['tip_pct', 'total_bill'] # Ahora tenemos una lista de columnas a diferencia del ejemplo anterior donde solo seleccionabamos\n","# una columna y a esa columna le aplicabamos varias funciones\n","functions = ['count', 'mean', 'max'] # A cada una de las columnas de la lista le aplicaremos entonces varias funciones\n","\n","result = grouped[columnas].agg(functions) # A las dos columnas del DF le aplicamos las tres funciones\n","\n","result"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pZ5JSdBRqLzX"},"source":["Ahora, supongamos que deseamos aplicar funciones potencialmente diferentes a una o más de las columnas. Para hacer esto, pasamos un dict a *agg* que contenga una asignación de nombres de columna a cualquiera de las especificaciones de funciones enumeradas hasta ahora"]},{"cell_type":"code","metadata":{"id":"0xc-DJrSqTS5"},"source":["grouped.agg({'tip' : np.max, 'size' : 'sum'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CgAUwiEbqbHu"},"source":["grouped.agg({'tip_pct' : ['min', 'max', 'mean', 'std'],\n","             'size' : 'sum'})"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K8sgtonjqmcI"},"source":["\n","---\n","\n","## Apply\n","\n","El método mas general de uso de GroupBy es **apply**.\n","\n","Como se ilustra en la Figura, **apply** divide el objeto que se está manipulando en piezas, invoca la función pasada en cada pieza y luego intenta concatenar las piezas juntas.\n","\n","![alt text](https://raw.githubusercontent.com/al34n1x/DataScience/master/img/split-apply-combine.png)\n","\n","*Fuente: Python for Data Analysis, 2nd Edition*"]},{"cell_type":"markdown","metadata":{"id":"u3MjcKzCrDoJ"},"source":["Supongamos que deseamos seleccionar los cinco valores principales de **tip_pct** por grupo. Primero, escribimos una función que seleccione las filas con los valores más grandes en una columna particular:"]},{"cell_type":"code","source":["# Volvamos a trabajar con el dataframe original\n","prop.head()"],"metadata":{"id":"zCoex4VR6F3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R7T43e2srIUu"},"source":["def top(df, n=5, column='tip_pct'):\n","  return df.sort_values(by=column)[-n:] # Está haciendo un sort por columna \"tip_pct\" y retornando las últimas \"n\" filas"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_XuuZpwPrOq1"},"source":["top(prop, n=6) # Llamada a la función top y reemplaza n=5 de la funcion por n=6"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zQPuyS4mrbAe"},"source":["Ahora, si agrupamos por fumador, por ejemplo, y llamamos a esta función, obtenemos lo siguiente:"]},{"cell_type":"code","metadata":{"id":"L-AxvCDardci"},"source":["prop.groupby('smoker').apply(top) # apply llama a la función top"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbzqRzSXrpli"},"source":["¿Qué ha pasado aquí? \n","\n","La función superior se llama en cada grupo de filas desde cada split del dataframe (el primer grupo es la agrupación smoker \"Yes\" y el segundo es la agrupación smoker \"No\") y luego los resultados se pegan usando *pandas.concat*, etiquetando las piezas con los nombres de los grupos. \n","\n","Por lo tanto, el resultado tiene un índice jerárquico cuyo nivel interno contiene valores de índice del DataFrame original."]},{"cell_type":"markdown","metadata":{"id":"XgR1Gdaer7qP"},"source":["Si pasas una función a *apply* que toma otros argumentos o palabras clave, puedes pasarlos después de la función:"]},{"cell_type":"code","metadata":{"id":"A3N2uRwer1rh"},"source":["prop.groupby(['smoker', 'day']).apply(top, n=2, column='total_bill') # En este caso aplicamos sobre un agrupamiento de dos claves."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mFfqx7P0sbeq"},"source":["\n","\n","---\n","\n","\n","### Análisis de cuantiles y buckets\n","\n","Pandas tiene algunas herramientas, en particular *cut* y *qcut*, para dividir los datos en cubos con contenedores de tu elección o por cuantiles de muestra. La combinación de estas funciones con *groupby* hace que sea conveniente realizar análisis de buckets o cuantiles en un conjunto de datos. Considere un conjunto de datos aleatorio simple y una categorización de bucket de igual longitud usando cut:"]},{"cell_type":"code","metadata":{"id":"TP3INoKgsqpP"},"source":["frame = pd.DataFrame({'data1': np.random.randn(1000),\n","                      'data2': np.random.randn(1000)})\n","frame"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0QizzlostA6"},"source":["quartiles = pd.cut(frame.data1, 4) # Cortamos los datos en 4 conjuntos"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q6-3UJN6swtW"},"source":["quartiles[:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJnuec9Os4c4"},"source":["El objeto  devuelto por *cut* se puede pasar directamente a *groupby*. Entonces podríamos calcular un conjunto de estadísticas para la columna data2 de la siguiente manera:"]},{"cell_type":"code","metadata":{"id":"Gmn-OJd_s-zC"},"source":["def get_stats(group):\n","  return {'min': group.min(), 'max': group.max(),\n","          'count': group.count(), 'mean': group.mean()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Cc-cqTttFds"},"source":["grouped = frame['data2'].groupby(quartiles)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VnM8DT4ltJz9"},"source":["grouped.apply(get_stats) # que puedo agregar para que se vea mejor? ..un___..?"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Entonces nos queda:\n","grouped.apply(get_stats).unstack()"],"metadata":{"id":"xVHUvF1w8kLe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tvKXddD9tV0C"},"source":["### Rellenar valores perdidos con valores específicos de grupo\n","\n","En clases anteriores vimos que a veces simplemente haremos **dropna** pero otras veces necesitaremos reemplazar los datos faltantes (nulos) por valores convenientes.\n","\n","*fillna* es la herramienta adecuada para usar; por ejemplo, aquí rellenamos los valores de NA con la media, como vimos previamente:"]},{"cell_type":"code","metadata":{"id":"KEghGmXztln2"},"source":["s = pd.Series(np.random.randn(6))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKZtrLBxtnbL"},"source":["s[:3] = np.nan\n","s"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YBnIaGGVt--k"},"source":["s.fillna(s.mean())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HUaBCYRUuKyv"},"source":["**Supongamos que necesitas que el valor de relleno varíe según el grupo.**\n","\n","Una forma de hacer esto es agrupar los datos y usar *apply* con una función que llame a *fillna* en cada fragmento de datos. \n","\n","Aquí hay algunos datos de muestra sobre los estados de EE. UU. Divididos en regiones orientales y occidentales:"]},{"cell_type":"code","metadata":{"id":"48_FJtkxuKbY"},"source":["states = ['Ohio', 'New York', 'Vermont', 'Florida',\n","          'Oregon', 'Nevada', 'California', 'Idaho']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xR-6KbEhuWCD"},"source":["# group_key = ['East'] * 4 + ['West'] * 4 # Notación alternativa\n","group_key = ['East', 'East', 'East', 'East', 'West', 'West', 'West', 'West']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fkyD6IrZuwIZ"},"source":["Ten en cuenta que la sintaxis ['Este'] * 4 produce una lista que contiene cuatro copias de los elementos en ['Este']."]},{"cell_type":"code","metadata":{"id":"tfB2c5CKudH8"},"source":["data = pd.Series(np.random.randn(8), index=states)\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"28UwIWF0ud2U"},"source":["data['Vermont', 'Nevada', 'Idaho'] = np.nan\n","data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yTq_O1UvBUl"},"source":["data.groupby(group_key).mean() # Al hacer el promedio, NO contempla los nulos, no los suma."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RmSiIdUiv2Al"},"source":["fill_mean = lambda g: g.fillna(g.mean()) # Que hace esta funcion lambda?"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vtgvWzauv3Kk"},"source":["data.groupby(group_key).apply(fill_mean)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eX6B4Plhxr8P"},"source":["\n","\n","---\n","\n","\n","### Un ejemplito de transformación y correlación entre columnas\n","\n","El tema de correlación no se verá más a fondo en este curso (para eso está la Diplo en Ciencia de Datos de UADE), pero veamos ahora una simple transformación.\n","\n","Consideremos un conjunto de datos financieros originalmente obtenido de Yahoo! Finance que contiene precios al final del día para algunas acciones y el índice S&P 500 (el símbolo SPX):"]},{"cell_type":"code","metadata":{"id":"vu2M02OHyBVM"},"source":["close_px = pd.read_csv('https://raw.githubusercontent.com/manquintana/Finlab-UADE/main/Ejercitacion/Datasets/stocks.csv', \n","                       parse_dates=True, index_col=0)\n","close_px.info()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xg0NfncTyd_D"},"source":["close_px"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xH-rN_cZyqjF"},"source":["Una tarea de interés podría ser calcular un DataFrame que consta de las correlaciones anuales de los rendimientos diarios con SPX. \n","\n","1° paso: Vamos a hacer una **transformación**:\n"]},{"cell_type":"code","metadata":{"id":"N2f0ofBzy2e4"},"source":["rets = close_px.pct_change().dropna() #Calculamos el procentaje de cambio y eliminamos nulos\n","# Por defecto, la función pct_change, calcula el porcentaje de cambio entre el valor actual y el de la row inmediata anterior"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos que en lugar de los valores originales, ahora tenemos el porcentaje de cambio\n","\n","¿Qué dato desapareció? ¿Tiene sentido?"],"metadata":{"id":"Z3qUt0NFCZkr"}},{"cell_type":"code","source":["rets"],"metadata":{"id":"9LOytWIsCFKO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["2° paso: Creamos una función que calcula la correlación por pares de cada columna con la columna 'SPX':"],"metadata":{"id":"0l7rD0-vDHbO"}},{"cell_type":"code","metadata":{"id":"4WTxhA2KyvpP"},"source":["spx_corr = lambda x: x.corrwith(x['SPX']) #Esta funcion aplica a Dataframes exclusivamente y mide la correlación entre cada columna con la que se pasa como parámetro"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyS6K-bdy9Id"},"source":["get_year = lambda x: x.year\n","by_year = rets.groupby(get_year) # Agrupamos los porcentajes de cambio por año\n","resultado = by_year.apply(spx_corr)\n","'''\n","Llama a la funcion spx_corr para calcular la correlación de cada columna del dataframe contra\n","la columna 'SPX', luego de hacer la agrupacion por año.\n","'''\n","resultado"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Si quisieramos ver la pinta que tiene una matriz de correlación completa para todas las columnas para alguno de los años, por ejemplo para 2003 (primer fila), podemos hacer:"],"metadata":{"id":"ACCkz127Fp8L"}},{"cell_type":"code","source":["resultado = resultado.loc[2003:2010,:]#Hago el loc que devuelve una serie y la transformo en dataframe\n","# Agrego \"transpose\" para obtener las filas como columnas\n","resultado"],"metadata":{"id":"P3isXQ8iEE8m"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Por último analicemos la correlación entre todas las variables del dataframe.\n","\n","- Antes usamos `corrwith` que calcula la correlación entre una columna específica y las demás.\n","\n","- Ahora con la función `corr` podemos calcular la correlación entre todas."],"metadata":{"id":"s-B7YO1Ep4F-"}},{"cell_type":"code","source":["resultado.corr()"],"metadata":{"id":"UYoQV2r1c6wI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3e5jR8qh0UMW"},"source":["\n","\n","---\n","\n","\n","## Pivot Tables y tabulación cruzada\n","\n","Una tabla dinámica es una herramienta de resumen de datos que se encuentra con frecuencia en programas de hojas de cálculo. \n","\n","Agrega una tabla de datos por una o más claves, organizando los datos en un rectángulo con algunas de las claves de grupo a lo largo de las filas y algunas a lo largo de las columnas. \n","\n","Las tablas dinámicas en Python con Pandas son posibles a través de la función *groupby*. DataFrame tiene un método *pivot_table* y también hay una función *pandas.pivot_table* de nivel superior. Además de proporcionar una interfaz conveniente para *groupby*, *pivot_table* puede agregar totales parciales, también conocidos como márgenes.\n","\n","Volviendo al conjunto de datos de propinas, supongamos que deseamos calcular una tabla de promedios grupales:"]},{"cell_type":"code","source":["prop.head()"],"metadata":{"id":"CKBihrnyHnCz"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dRbo_Gfj0sY3"},"source":["prop.pivot_table(index=['day', 'smoker'])\n","# En este caso estamos generando una agrupación con promedios por columna, \n","# y lo que obtenemos es un dataframe con índices jerárquicos"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9sD_c5SG006b"},"source":["Ahora, supongamos que queremos agregar solo *tip_pct* y *size*, y además agrupar por tiempo. \n","\n","Pondremos fumador en las columnas de la tabla y día en las filas:"]},{"cell_type":"code","metadata":{"id":"PIGvHbai1Avl"},"source":["prop.pivot_table(['tip_pct', 'size'], index=['time', 'day'],\n","                 columns='smoker')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKEcTk2q1X3U"},"source":["prop.pivot_table('tip_pct', index=['time', 'size', 'smoker'],\n","                 columns='day', aggfunc='mean', fill_value=0) # Si hay NaN podemos usar fill_value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gprrjJ0m12nf"},"source":["\n","### Tabulaciones cruzadas (crosstab)\n","Una tabulación cruzada es un caso especial de una tabla dinámica que **calcula las frecuencias de grupo**. Aquí hay un ejemplo:"]},{"cell_type":"code","metadata":{"id":"lTGc0xNm3JnY"},"source":["pd.crosstab([prop.time, prop.day], prop.smoker)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ekJQR3hh3ARM"},"source":["Podríamos aumentar esta tabla para incluir totales parciales pasando 'margins=True'. Esto tiene el efecto de agregar todas las etiquetas de fila y columna, siendo los valores correspondientes las estadísticas de grupo para todos los datos dentro de un solo nivel"]},{"cell_type":"code","metadata":{"id":"h9jNuUUX1_dF"},"source":["df_cross = pd.crosstab([prop.time, prop.day], prop.smoker, margins=True)\n","df_cross"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Si se quiere acceder a un elemento dentro de un indice jerarquico, \n","# refinamos por columna y luego por indice con la jerarquia\n","df_cross = df_cross[['All']].loc['Dinner','Fri']\n","df_cross"],"metadata":{"id":"d8iWXEEie5EB"},"execution_count":null,"outputs":[]}]}